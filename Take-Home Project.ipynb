{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Because the CSV file is not rectangular, Pandas won't load the file without specifiying column names.\n",
    "#I will name the columns 'c0','c1','c2'...,'c3391' (the longest row in the CSV file has 3392 columns).\n",
    "df = pd.read_csv('wikipedia_machine_learning.csv', names=['c' + str(i) for i in range(3392)], dtype='str', encoding='utf-8')\\\n",
    "       .fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For ease of text analysis, all of the columns for a given row will be concatenated into a single variable, 'columns_combined'.\n",
    "df['columns_combined'] = ''\n",
    "for j in range(df.shape[1]-1):\n",
    "    df['columns_combined'] += df.loc[:, 'c'+str(j)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will proceed by building various functions that will allow me to create the following new columns from the existing columns of df:\n",
    "\n",
    "• *wikipedia_url* (of the article) <br />\n",
    "• *article_title* <br />\n",
    "• *section_titles* (in the article) <br />\n",
    "• *subsection_titles* (in the article) <br />\n",
    "• *first_sentence* (of the article) <br />\n",
    "• *article_length* (number of characters) <br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I ended up not using this function.\n",
    "def wikipedia_url(string):\n",
    "    \"\"\"\n",
    "    Returns the URL of the Wikipedia article, usually embedded in the first column.\n",
    "    \"\"\"\n",
    "    #The URL's are contained within '\\t....\\t', and re.findall() is saved as a list, hence the [0].\n",
    "    url = re.findall(r'\\t.+?\\t', string)[0]\n",
    "    #We don't want the \\t's returned, so we eliminate the first and last characters from the url.\n",
    "    #Also, the 'https://' substring is saving the strings as URLs, truncating some URL ends, leading to erroneous pages.\n",
    "    #Therefore I will remove these characters as well.\n",
    "    return url[9:len(url)-1]\n",
    "\n",
    "#Add the new 'url' variable to df\n",
    "df['url'] = df['columns_combined'].apply(wikipedia_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def article_title(string):\n",
    "    \"\"\"\n",
    "    Returns the title of the Wikipedia article, usually embedded in the first column.\n",
    "    \"\"\"\n",
    "    #The article titles are at the beginning of the string, before the first '\\t'. Also, title is saved as a list, hence the [0].\n",
    "    title = re.findall(r'.+?\\t', string)[0]\n",
    "    #We don't want to return the \\t, so we eliminate the last character.\n",
    "    return title[:len(title)-1]\n",
    "\n",
    "#Add the new 'article_title' variable to df.\n",
    "#Usually the article title is in c0. However, it sometimes spills over to c1 and c2 as well.\n",
    "df['article_title'] = (df['c0'] + df['c1'] + df['c2']).apply(article_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def article_section_titles(string):\n",
    "    \"\"\"\n",
    "    Input: the full article text, found in df['columns_combined'].\n",
    "    Output: a list of section titles for the input Wikipedia page.\n",
    "    Typically, the section titles of an article appear within 4 equal signs, with a space before the first, a space after\n",
    "    the second and no space before the third.\n",
    "    e.g. ' == Types of artificial intelligence=='\n",
    "    \"\"\"\n",
    "    #Some of the pages have no section titles and need to be treated differently for string methods to work.\n",
    "    #We will distinguish these 2 cases by the number of occurrences of substrings of the form ' == ...=='.\n",
    "    section_titles = re.findall(r' == .+?==', string)\n",
    "    \n",
    "    if len(section_titles) > 0:\n",
    "    #The = signs will be removed, as well as the 2 spaces at the beginning of each section title.\n",
    "        section_titles = list(pd.Series(section_titles)\\\n",
    "                                .str.replace('=','')\\\n",
    "                                .str.lstrip(' '))\n",
    "        return section_titles\n",
    "    \n",
    "    if len(section_titles) == 0:\n",
    "    #These rows will return an error if we try to apply the string methods from the above case, so we treat them separately.\n",
    "        return []\n",
    "\n",
    "#Add the new 'section_titles' variable to df.\n",
    "df['section_titles'] = df['columns_combined'].apply(article_section_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def article_subsection_titles(string):\n",
    "    \"\"\"\n",
    "    Input: the full article text, found in df['columns_combined'].\n",
    "    Output: a list of subsection titles for the input Wikipedia page.\n",
    "    Typically, the section titles of an article appear within 6 equal signs, with a space before the first, a space after\n",
    "    the third and no space before the fourth.\n",
    "    e.g. ' === Metric==='\n",
    "    \"\"\"\n",
    "    #Some of the pages have no subsection titles and need to be treated differently for string methods to work.\n",
    "    #We will distinguish these 2 cases by the number of occurrences of substrings of the form ' === ...==='.\n",
    "    subsection_titles = re.findall(r' === .+?===', string)\n",
    "    \n",
    "    if len(subsection_titles) > 0:\n",
    "    #The = signs will be removed, as well as the 2 spaces at the beginning of each subsection title.\n",
    "        subsection_titles = list(pd.Series(subsection_titles)\\\n",
    "                                   .str.replace('=','')\\\n",
    "                                   .str.lstrip(' '))\n",
    "        return subsection_titles\n",
    "    \n",
    "    if len(subsection_titles) == 0:\n",
    "    #These rows will return an error if we try to apply the string methods from the above case, so we treat them separately.\n",
    "        return []\n",
    "    \n",
    "#Add the new 'subsection_titles' variable to df.\n",
    "df['subsection_titles'] = df['columns_combined'].apply(article_subsection_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def article_first_sentence(string):\n",
    "    \"\"\"\n",
    "    Returns the first sentence of the Wikipedia article, identified by the first period after the URL.\n",
    "    Note that the commas from the Wikipedia article will not appear.\n",
    "    \"\"\"\n",
    "    #For some articles there is no sentence in the dataset, and these articles need to be processed separately.\n",
    "    #For these articles there will be at most 1 substring of the form '\\t.+?\\.', namely '\\thttps://en.'from the URL.\n",
    "    \n",
    "    substring = re.findall(r'\\t.+?\\.', string)\n",
    "    \n",
    "    if len(substring) > 1:\n",
    "        #Ignore the first matched string, '\\thttps://en.', and skip to the second.\n",
    "        string_starting_with_second_tab = substring[1]\n",
    "        #Sometimes the second \\t is followed by \", and then the first sentence begins.\n",
    "        if string_starting_with_second_tab[1] == '\"':\n",
    "            #Return the string without the \\t\".\n",
    "            return string_starting_with_second_tab[2:]\n",
    "        #Other times it begins immediately after the second \\t, without \".\n",
    "        else:\n",
    "            #Return the string without the \\t.\n",
    "            return string_starting_with_second_tab[1:]\n",
    "        \n",
    "    if len(substring) <= 1:\n",
    "        #There is no sentence in the article.\n",
    "        return ''\n",
    "\n",
    "#Add the new 'first_sentence' variable to df.  \n",
    "df['first_sentence'] = df['columns_combined'].apply(article_first_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I ended up not using this function.\n",
    "def article_length(string):\n",
    "    \"\"\"\n",
    "    This returns the number of characters in the dataset for the input article.\n",
    "    \"\"\"\n",
    "    return len(string)\n",
    "\n",
    "#Add the new 'article_length' variable to df.\n",
    "df['article_length'] = df['columns_combined'].apply(article_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>columns_combined</th>\n",
       "      <th>url</th>\n",
       "      <th>article_title</th>\n",
       "      <th>section_titles</th>\n",
       "      <th>subsection_titles</th>\n",
       "      <th>first_sentence</th>\n",
       "      <th>article_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Outline of artificial intelligence\\thttps://en...</td>\n",
       "      <td>en.wikipedia.org/wiki/Outline_of_artificial_in...</td>\n",
       "      <td>Outline of artificial intelligence</td>\n",
       "      <td>[What type of thing is artificial intelligence...</td>\n",
       "      <td>[By approach, By application, Integrated AI sy...</td>\n",
       "      <td>The following outline is provided as an overvi...</td>\n",
       "      <td>24206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Outline of computer vision\\thttps://en.wikiped...</td>\n",
       "      <td>en.wikipedia.org/wiki/Outline_of_computer_vision</td>\n",
       "      <td>Outline of computer vision</td>\n",
       "      <td>[Branches of computer vision, History of compu...</td>\n",
       "      <td>[Image enhancement, Transformations, Filtering...</td>\n",
       "      <td>The following outline is provided as an overvi...</td>\n",
       "      <td>4431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Outline of natural language processing\\thttps:...</td>\n",
       "      <td>en.wikipedia.org/wiki/Outline_of_natural_langu...</td>\n",
       "      <td>Outline of natural language processing</td>\n",
       "      <td>[Natural language processing, Prerequisite tec...</td>\n",
       "      <td>[Applications, Component processes, Timeline o...</td>\n",
       "      <td>The following outline is provided as an overvi...</td>\n",
       "      <td>49401</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    columns_combined  \\\n",
       "0  Outline of artificial intelligence\\thttps://en...   \n",
       "1  Outline of computer vision\\thttps://en.wikiped...   \n",
       "2  Outline of natural language processing\\thttps:...   \n",
       "\n",
       "                                                 url  \\\n",
       "0  en.wikipedia.org/wiki/Outline_of_artificial_in...   \n",
       "1   en.wikipedia.org/wiki/Outline_of_computer_vision   \n",
       "2  en.wikipedia.org/wiki/Outline_of_natural_langu...   \n",
       "\n",
       "                            article_title  \\\n",
       "0      Outline of artificial intelligence   \n",
       "1              Outline of computer vision   \n",
       "2  Outline of natural language processing   \n",
       "\n",
       "                                      section_titles  \\\n",
       "0  [What type of thing is artificial intelligence...   \n",
       "1  [Branches of computer vision, History of compu...   \n",
       "2  [Natural language processing, Prerequisite tec...   \n",
       "\n",
       "                                   subsection_titles  \\\n",
       "0  [By approach, By application, Integrated AI sy...   \n",
       "1  [Image enhancement, Transformations, Filtering...   \n",
       "2  [Applications, Component processes, Timeline o...   \n",
       "\n",
       "                                      first_sentence  article_length  \n",
       "0  The following outline is provided as an overvi...           24206  \n",
       "1  The following outline is provided as an overvi...            4431  \n",
       "2  The following outline is provided as an overvi...           49401  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[:3, 3392:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(column):\n",
    "    \"\"\"\n",
    "    This function preprocesses a column of strings (e.g. 'article_title') or a column of lists of strings (e.g. \n",
    "    'section_titles'). The input is a column of df written as a string (e.g. 'article_title' or 'section_titles'). \n",
    "    Preprocessing consists of removing punctuation, converting to lower-case and removing stop words. The output is a \n",
    "    series of all of the words that occur among all the rows of the input column, where each entry is a single word.\n",
    "    \"\"\"\n",
    "    \n",
    "    #The entries of 'section_titles' and 'subsection_titles' are lists of strings. These columns need to be converted\n",
    "    #to a single list for the following preprocessing steps to work.\n",
    "    if column in ['section_titles', 'subsection_titles']:\n",
    "        #Combine the lists into a single list.\n",
    "        L = []\n",
    "        for i in range(df.shape[0]):\n",
    "            L += df.loc[i, column]\n",
    "        #Combine the list entries (strings) into a single string\n",
    "        string = ''\n",
    "        for i in range(len(L)):\n",
    "            string += ' ' + L[i]\n",
    "            \n",
    "    #The entries of 'article_title', 'first_sentence' and 'columns_combined' are strings.\n",
    "    else:\n",
    "        #Combine the strings into a single string.\n",
    "        string = ''\n",
    "        for i in range(df.shape[0]):\n",
    "            string += ' ' + df.loc[i, column]\n",
    "    \n",
    "    #Tokenize string into words and remove punctuation.\n",
    "    word_list = nltk.RegexpTokenizer(r'\\w+')\\\n",
    "                    .tokenize(string)\n",
    "    \n",
    "    #Convert words to lower-case.\n",
    "    word_list = [word.lower() for word in word_list]\n",
    "    \n",
    "    #Remove stop words.\n",
    "    #These are default stop words.\n",
    "    stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "    #These are additional stop words I have chosen by looking through the most common words in 'section_titles'.\n",
    "    extra_stop_words = ['see', 'references', 'also', 'links', 'external', 'history', 'reading', 'notes', 'examples', \n",
    "                        'definition', 'overview', 'example', 'related', 'bibliography', 'use', 'users', 'legal', 'two']\n",
    "    for word in extra_stop_words:\n",
    "        stopwords.add(word)\n",
    "    #The removal.\n",
    "    word_list = [word for word in word_list if word not in stopwords]\n",
    "    \n",
    "    #Convert to a series so that we can apply Pandas methods to the output.\n",
    "    return pd.Series(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenated_ngrams(preprocessed_column, n):\n",
    "    \"\"\"\n",
    "    This function takes a string as an input, and is intended specifically to take an output from preprocess() as its\n",
    "    input. It returns the ngrams of a column for n = 2 or 3, a series of strings where each string consists of n words.\n",
    "    \"\"\"\n",
    "    if n == 2:\n",
    "        #Create the bigrams.\n",
    "        ngrams = list(nltk.ngrams(preprocessed_column, 2))\n",
    "        #ngrams is a list of 2-tuples. Combine each pair of elements into a string.\n",
    "        L = []\n",
    "        for w1,w2 in ngrams:\n",
    "            L.append(w1 + ' ' + w2)\n",
    "        #Convert to a series.\n",
    "        return pd.Series(L)\n",
    "    \n",
    "    if n == 3:\n",
    "        #Create the 3-grams.\n",
    "        ngrams = list(nltk.ngrams(preprocessed_column, 3))\n",
    "        #ngrams is a list of 3-tuples. Combine each triplet of elements into a string.\n",
    "        L = []\n",
    "        for w1,w2,w3 in ngrams:\n",
    "            L.append(w1 + ' ' + w2 + ' ' + w3)\n",
    "        #Convert to a series.\n",
    "        return pd.Series(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ARTICLE TITLES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing the article_title column.\n",
    "article_title_preprocessed = preprocess('article_title')\n",
    "\n",
    "#Counting and sorting the most common words among all the rows in article_title.\n",
    "titles_words_tallied = article_title_preprocessed.value_counts()\\\n",
    "                                                 .sort_values(ascending=False)\n",
    "\n",
    "#Counting and sorting the most common bigrams among all the rows in article_title.\n",
    "titles_2grams_tallied = concatenated_ngrams(article_title_preprocessed, 2).value_counts()\\\n",
    "                                                                          .sort_values(ascending=False)\n",
    "\n",
    "#Counting and sorting the most common trigrams among all the rows in article_title.\n",
    "titles_3grams_tallied = concatenated_ngrams(article_title_preprocessed, 3).value_counts()\\\n",
    "                                                                          .sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         outline\n",
       "1      artificial\n",
       "2    intelligence\n",
       "dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#An example showing how an output of preprocess() looks.\n",
    "article_title_preprocessed[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list        225\n",
       "analysis    146\n",
       "theory      133\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#An example showing how a ..._words_tallied object looks.\n",
    "titles_words_tallied[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "artificial intelligence    33\n",
       "machine learning           26\n",
       "neural network             25\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#An example showing how a ..._2grams_tallied object looks.\n",
    "titles_2grams_tallied[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "natural language processing     7\n",
       "buffalo buffalo buffalo         6\n",
       "principal component analysis    5\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#An example showing how a ..._3grams_tallied object looks.\n",
    "titles_3grams_tallied[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SECTION TITLES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing the section_titles column.\n",
    "section_titles_preprocessed = preprocess('section_titles')\n",
    "\n",
    "#Counting and sorting the most common words among all the rows in section_titles.\n",
    "section_titles_words_tallied = section_titles_preprocessed.value_counts()\\\n",
    "                                                          .sort_values(ascending=False)\n",
    "\n",
    "#Counting and sorting the most common bigrams among all the rows in section_titles.    \n",
    "section_titles_2grams_tallied = concatenated_ngrams(section_titles_preprocessed, 2).value_counts()\\\n",
    "                                                                                   .sort_values(ascending=False)\n",
    "\n",
    "#Counting and sorting the most common trigrams among all the rows in section_titles.    \n",
    "section_titles_3grams_tallied = concatenated_ngrams(section_titles_preprocessed, 3).value_counts()\\\n",
    "                                                                                   .sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SUBSECTION TITLES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing the subsection_titles column.\n",
    "subsection_titles_preprocessed = preprocess('subsection_titles')\n",
    "\n",
    "#Counting and sorting the most common words among all the rows in subsection_titles.\n",
    "subsection_titles_words_tallied = subsection_titles_preprocessed.value_counts()\\\n",
    "                                                                .sort_values(ascending=False)\n",
    "    \n",
    "#Counting and sorting the most common bigrams among all the rows in subsection_titles.\n",
    "subsection_titles_2grams_tallied = concatenated_ngrams(subsection_titles_preprocessed, 2).value_counts()\\\n",
    "                                                                                         .sort_values(ascending=False)\n",
    "\n",
    "#Counting and sorting the most common trigrams among all the rows in subsection_titles.    \n",
    "subsection_titles_3grams_tallied = concatenated_ngrams(subsection_titles_preprocessed, 3).value_counts()\\\n",
    "                                                                                         .sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FIRST SENTENCES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing the first_sentence column.\n",
    "first_sentences_preprocessed = preprocess('first_sentence')\n",
    "\n",
    "#Counting and sorting the most common words among all the rows in first_sentence.\n",
    "first_sentences_words_tallied = preprocess('first_sentence').value_counts()\\\n",
    "                                                            .sort_values(ascending=False)\n",
    "\n",
    "#Counting and sorting the most common bigrams among all the rows in first_sentence.\n",
    "first_sentence_2grams_tallied = concatenated_ngrams(first_sentences_preprocessed, 2).value_counts()\\\n",
    "                                                                                    .sort_values(ascending=False)\n",
    "\n",
    "#Counting and sorting the most common trigrams among all the rows in first_sentence.    \n",
    "first_sentence_3grams_tallied = concatenated_ngrams(first_sentences_preprocessed, 3).value_counts()\\\n",
    "                                                                                    .sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**COLUMNS COMBINED**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing the columns_combined column.\n",
    "columns_combined_preprocessed = preprocess('columns_combined')\n",
    "\n",
    "#Counting and sorting the most common words among all the rows in columns_combined.\n",
    "columns_combined_words_tallied = columns_combined_preprocessed.value_counts()\\\n",
    "                                                              .sort_values(ascending=False)\n",
    "\n",
    "#Counting and sorting the most common bigrams among all the rows in columns_combined.\n",
    "columns_combined_2grams_tallied = concatenated_ngrams(columns_combined_preprocessed, 2).value_counts()\\\n",
    "                                                                                       .sort_values(ascending=False)\n",
    "\n",
    "#Counting and sorting the most common trigrams among all the rows in columns_combined.    \n",
    "columns_combined_3grams_tallied = concatenated_ngrams(columns_combined_preprocessed, 3).value_counts()\\\n",
    "                                                                                       .sort_values(ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
