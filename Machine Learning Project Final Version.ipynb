{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Because the CSV file is not rectangular, Pandas won't load the file without specifiying column names.\n",
    "#I will name the columns 'c0','c1','c2'...,'c3391' (the longest row in the CSV file has 3392 columns).\n",
    "df = pd.read_csv('wikipedia_machine_learning.csv', names=['c' + str(i) for i in range(3392)], dtype='str', encoding='utf-8')\\\n",
    "       .fillna('')\n",
    "    \n",
    "#All of the columns for a given row will be concatenated into a single string, 'columns_combined'.\n",
    "df['columns_combined'] = ''\n",
    "for j in range(df.shape[1]-1):\n",
    "    df['columns_combined'] += df.loc[:, 'c'+str(j)]\n",
    "    \n",
    "#Save all of the Wikipedia data as a string list of length 7318 for later user.\n",
    "wiki = df['columns_combined'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def article_title(string):\n",
    "    \"\"\"\n",
    "    Returns the title of the Wikipedia article in df, usually embedded in the first column.\n",
    "    \"\"\"\n",
    "    #The article titles are at the beginning of the string, before the first '\\t'. Also, title is saved as a list, hence the [0].\n",
    "    title = re.findall(r'.+?\\t', string)[0]\n",
    "    #We don't want to return the \\t, so we eliminate the last character.\n",
    "    return title[:len(title)-1]\n",
    "\n",
    "#Add the new 'article_title' variable to df.\n",
    "#Usually the article title is in c0. However, it sometimes spills over to c1 and c2 as well.\n",
    "df['article_title'] = (df['c0'] + df['c1'] + df['c2']).apply(article_title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LexRank Summarizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wikipedia article title: Random close pack\n",
      "Top 3 sentences: \n",
      "\n",
      "1) Random close pack\thttps://en.wikipedia.org/wiki/Random_close_pack\t\"Random close packing(RCP) is an empirical parameter used to characterize the maximum volume fraction of solid objects obtained when they are packed randomly.\n",
      "\n",
      "2) But shaking cannot increase the density indefinitely a limit is reached and if this is reached without obvious packing into a regular crystal lattice this is the empirical random close-packed density.\n",
      "\n",
      "3) The definition of packing fraction can be given as: \"\"the volume taken by number of particles in a given space of volume\"\".\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sumy\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
    "\n",
    "def lex_rank_summarizer(string, n=1):\n",
    "    \"\"\"\n",
    "    For text summarization; this returns the top n most relevant sentences from a string of sentences. It uses LexRank, which is\n",
    "    an algorithm that uses word2vec to compute the pairwise similarities between each of the sentences and then ranks them.\n",
    "    \"\"\"\n",
    "    parser = PlaintextParser.from_string(string,Tokenizer(\"english\"))\n",
    "    summarizer = LexRankSummarizer()\n",
    "    summary = summarizer(parser.document, n)   \n",
    "    sentence_list = [str(summary[i]) for i in range(n)]\n",
    "    \n",
    "    return sentence_list\n",
    "\n",
    "\n",
    "#Example of lex_rank_summarizer() usage: the 3 most relevant sentences.\n",
    "article_title = df.loc[7317, 'article_title']\n",
    "print('Wikipedia article title: ' + article_title)\n",
    "\n",
    "doc = wiki[7317] #The Wikipedia article with row index of 7317.\n",
    "n = 3\n",
    "top_n_sentences = lex_rank_summarizer(doc, n)\n",
    "print('Top ' + str(n) + ' sentences: ' + '\\n')\n",
    "for i in range(n):\n",
    "    print(str(i+1) + ') ' + top_n_sentences[i] + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Luhn Summarizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wikipedia article title: Random close pack\n",
      "Top 3 sentences: \n",
      "\n",
      "1) For example when a solid container is filled with grain shaking the container will reduce the volume taken up by the objects thus allowing more grain to be added to the container.\n",
      "\n",
      "2) But shaking cannot increase the density indefinitely a limit is reached and if this is reached without obvious packing into a regular crystal lattice this is the empirical random close-packed density.\n",
      "\n",
      "3) Thus RCP is the packing fraction given by the limit as the tapping amplitude goes to zero and the limit as the number of taps goes to infinity.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sumy\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.luhn import LuhnSummarizer\n",
    "\n",
    "def luhn_summarizer(string, n=1):\n",
    "    \"\"\"\n",
    "    For text summarization; this returns the top n most relevant sentences from a string of sentences. \n",
    "    It uses Luhn, a heuristic method.\n",
    "    \"\"\"    \n",
    "    parser = PlaintextParser.from_string(string,Tokenizer(\"english\"))\n",
    "    summarizer_luhn = LuhnSummarizer()\n",
    "    summary = summarizer_luhn(parser.document,n)\n",
    "    sentence_list = [str(summary[i]) for i in range(n)]\n",
    "    \n",
    "    return sentence_list\n",
    "\n",
    "\n",
    "#Example of luhn_summarizer() usage: the 3 most relevant sentences.\n",
    "article_title = df.loc[7317, 'article_title']\n",
    "print('Wikipedia article title: ' + article_title)\n",
    "\n",
    "doc = wiki[7317] #The Wikipedia article with row index of 7317.\n",
    "n = 3\n",
    "top_n_sentences = luhn_summarizer(doc, n)\n",
    "print('Top ' + str(n) + ' sentences: ' + '\\n')\n",
    "for i in range(n):\n",
    "    print(str(i+1) + ') ' + top_n_sentences[i] + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Latent Semantic Analysis Summarizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wikipedia article title: Random close pack\n",
      "Top 3 sentences: \n",
      "\n",
      "1) While a conventional CCA generalizes principal component analysis(PCA) to two sets of random variables a gCCA generalizes PCA to more than two sets of random variables.\n",
      "\n",
      "2) They can always be made to vanish by introducing a new regression parameter for each common factor.\n",
      "\n",
      "3) Strother S.C.; Soltanian-Zadeh H.(2012) \"\"Enhancing reproducibility of fMRI statistical maps using generalized canonical correlation analysis in NPAIRS framework\"\" NeuroImage 60(4): 1970â€“1981.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sumy\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lsa import LsaSummarizer\n",
    "\n",
    "def lsa_summarizer(string, n=1):\n",
    "    \"\"\"\n",
    "    For text summarization; this returns the top n most relevant sentences from a string of sentences. \n",
    "    It uses Latent Semantic Analysis.\n",
    "    \"\"\"    \n",
    "    parser = PlaintextParser.from_string(string,Tokenizer(\"english\"))\n",
    "    summarizer_lsa = LsaSummarizer()\n",
    "    summary = summarizer_lsa(parser.document,n)\n",
    "    sentence_list = [str(summary[i]) for i in range(n)]\n",
    "    \n",
    "    return sentence_list\n",
    "\n",
    "#Example of lsa_summarizer() usage: the 3 most relevant sentences.\n",
    "article_title = df.loc[7317, 'article_title']\n",
    "print('Wikipedia article title: ' + article_title)\n",
    "\n",
    "doc = wiki[2019] #The Wikipedia article with row index of 7317.\n",
    "n = 3\n",
    "top_n_sentences = lsa_summarizer(doc, n)\n",
    "print('Top ' + str(n) + ' sentences: ' + '\\n')\n",
    "for i in range(n):\n",
    "    print(str(i+1) + ') ' + top_n_sentences[i] + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TextRank**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['calculus',\n",
       " 'functions',\n",
       " 'function',\n",
       " 'infinitesimal',\n",
       " 'infinitesimals',\n",
       " 'infinitesimally',\n",
       " 'mathematical',\n",
       " 'mathematics',\n",
       " 'mathematically',\n",
       " 'integral',\n",
       " 'integration',\n",
       " 'integrals',\n",
       " 'integrated',\n",
       " 'leibniz',\n",
       " 'differential',\n",
       " 'differentiation',\n",
       " 'time',\n",
       " 'times',\n",
       " 'newton',\n",
       " 'limit',\n",
       " 'limits',\n",
       " 'limiting']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#gensim is for topic modeling and similarity retrieval.\n",
    "from gensim.summarization.summarizer import summarize\n",
    "from gensim.summarization import keywords \n",
    "\n",
    "#Used for retrieving text from a webpage.\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    " \n",
    "def text_rank_summarizer(url, n=10):\n",
    "    \"\"\" \n",
    "    This returns the n most relevant terms from a webpage, unless there are less than n, in which case all are returned.\n",
    "    The terms are in descending order of relevance. Note that a group of words with a common root can be returned but only \n",
    "    count as 1, so the returned list can actually be longer than n.\n",
    "    \"\"\"\n",
    "    page = urlopen(url)\n",
    "    soup = BeautifulSoup(page, \"lxml\")\n",
    "    text = ' '.join(map(lambda p: p.text, soup.find_all('p')))\n",
    "    \n",
    "    #We need to count the number of terms so that we can use it in the min() function. Otherwise, if we input a value of\n",
    "    #n which is too large for the given URL then an error will be returned.\n",
    "    number_terms = len(keywords(str(text)).split('\\n'))\n",
    "    \n",
    "    #Retrieving the relevant terms, saved as a list.\n",
    "    relevant_terms = keywords(str(text), words=min(n, number_terms)).split('\\n')\n",
    "    \n",
    "    return relevant_terms\n",
    "\n",
    "\n",
    "#Example of relevant_terms() usage. Note that the terms are lists in descending order of relevance.\n",
    "text_rank_summarizer(\"https://en.wikipedia.org/wiki/Calculus\", n=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
